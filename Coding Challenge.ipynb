{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Will it snow tomorrow?\" - The time traveler asked\n",
    "The following dataset contains climate information form over 9000 stations accross the world. The overall goal of these subtasks will be to predict whether it will snow tomorrow 13 years ago. So if today is 2022.02.15 then the weather we want to forecast is for the date 2009.02.16. You are suppsed to solve the tasks using Big Query, which can be used in the Jupyter Notebook like it is shown in the following cell. For further information and how to used BigQuery in Jupyter Notebook refer to the Google Docs. \n",
    "\n",
    "The goal of this test is, to test your coding knowledge in Python, BigQuery and Pandas as well as your understanding of Data Science. If you get stuck at the first part, you can use the replacement data provided in the second part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "To solve the task given above, we roughly follow the following procedure: \n",
    "\n",
    "1. Load relevant data & apply filters mentioned in the task descriptions\n",
    "2. Inspect data and select features to be used for prediction\n",
    "3. Split data and train a selection of classifiers on the training set\n",
    "4. Use the best performing model to predict whether it is snowing on target day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Bigquery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The google.cloud.bigquery extension is already loaded. To reload it, use:\n",
      "  %reload_ext google.cloud.bigquery\n"
     ]
    }
   ],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Task\n",
    "Change the date format to 'YYYY-MM-DD' and select the data from 2005 till 2009 for station numbers including and between 725300 and 726300 , and save it as a pandas dataframe. Note the maximum year available is 2010. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bigquery magic decorator to work we need to define a GCP project. We use *learnings-coding-challenge*, the project created for this task. By putting a name after the decorator, the query result is automatically saved into a pandas.DataFrame object.\n",
    "\n",
    "We retrieve all columns that fulfill the conditions posed in the task and create an additional column called *date* that contains the date in *'Y-M-D'* format. The *BETWEEN* operator is inclusive and thus stations 725300 and 726300 are included in the table returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 415.81query/s] \n",
      "Downloading: 100%|██████████| 377784/377784 [00:07<00:00, 51321.53rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery df --project learnings-coding-challenge\n",
    "SELECT \n",
    "*, \n",
    "DATE(year, month, day) AS date\n",
    "FROM `bigquery-public-data.samples.gsod`\n",
    "WHERE year BETWEEN 2005 AND 2009\n",
    "AND station_number BETWEEN 725300 AND 726300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Task \n",
    "**From here want to work with the data from all stations 725300 to 725330 that have information from 2005 till 2009.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here continue using Python - more specifically using pandas to manipulate the DataFrame we just retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep data where station_number is within given range\n",
    "relevant_stations = df[(df['station_number']>725299)&(df['station_number']<725331)]\n",
    "#only keep data of stations that have data in each year\n",
    "#we have five years\n",
    "years = 5\n",
    "sn_years = relevant_stations[['station_number', 'year']].groupby(['station_number']).nunique()\n",
    "#only keep the stations where count of years == years\n",
    "stations_all_years = list(sn_years[sn_years['year']==years].index)\n",
    "stations_cleaned = relevant_stations[relevant_stations['station_number'].isin(stations_all_years)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, all stations within the range of station numbers we are interested in appear in each of the relevant years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do a first analysis of the remaining dataset, clean or drop data depending on how you see appropriate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is an important step to improve a model's performance. Given the time constraint and the fact that machine learning estimators used further below can handle high-dimensional data better than linear estimators such as Logistic Regression, we restrict ourselves to a few simple steps here. \n",
    "\n",
    "We start by investigating the percentage of missing observations across all features. Below the percentage of missings of each available variable is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_number                          0.000000\n",
      "wban_number                             0.000000\n",
      "year                                    0.000000\n",
      "month                                   0.000000\n",
      "day                                     0.000000\n",
      "mean_temp                               0.000000\n",
      "num_mean_temp_samples                   0.000000\n",
      "mean_dew_point                          0.011032\n",
      "num_mean_dew_point_samples              0.011032\n",
      "mean_sealevel_pressure                 10.138452\n",
      "num_mean_sealevel_pressure_samples     10.138452\n",
      "mean_station_pressure                  94.522588\n",
      "num_mean_station_pressure_samples      94.522588\n",
      "mean_visibility                         0.016548\n",
      "num_mean_visibility_samples             0.016548\n",
      "mean_wind_speed                         0.027580\n",
      "num_mean_wind_speed_samples             0.027580\n",
      "max_sustained_wind_speed                0.055160\n",
      "max_gust_wind_speed                    36.742236\n",
      "max_temperature                         0.011032\n",
      "max_temperature_explicit                0.011032\n",
      "min_temperature                       100.000000\n",
      "min_temperature_explicit              100.000000\n",
      "total_precipitation                     1.053561\n",
      "snow_depth                             96.883446\n",
      "fog                                     0.000000\n",
      "rain                                    0.000000\n",
      "snow                                    0.000000\n",
      "hail                                    0.000000\n",
      "thunder                                 0.000000\n",
      "tornado                                 0.000000\n",
      "date                                    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#what's percentage of NaN for each feature?\n",
    "missing_pct = stations_cleaned.isnull().sum() * 100 / len(stations_cleaned)\n",
    "print(missing_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that multiple variables only contain missing values in the dataframe we have created by applying the imposed restrictions. Hence, we can remove these variables as well as other variables that have a high percentage of missing values. We set the treshold to 90% percent (i.e. observations with less than 10% missings are kept). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['station_number', 'wban_number', 'year', 'month', 'day', 'mean_temp', 'num_mean_temp_samples', 'mean_dew_point', 'num_mean_dew_point_samples', 'mean_visibility', 'num_mean_visibility_samples', 'mean_wind_speed', 'num_mean_wind_speed_samples', 'max_sustained_wind_speed', 'max_temperature', 'max_temperature_explicit', 'total_precipitation', 'fog', 'rain', 'snow', 'hail', 'thunder', 'tornado', 'date']\n"
     ]
    }
   ],
   "source": [
    "#only keep variables with more than 90% of observations (just a random threshold really)\n",
    "keep_cols = list(missing_pct[missing_pct<10].index)\n",
    "low_missings = stations_cleaned[keep_cols]\n",
    "print(keep_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can rule out some features by taking a closer look at the documentation of the data (found here: https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=samples&t=gsod&page=table&_ga=2.176688416.426819128.1655642879-296738449.1655457946&project=learnings-coding-challenge).\n",
    "Some features contained in the data are meta-data that are more than unlikely to contain valuable information that helps us to predict whether it will snow. For example, *num_mean_temp_samples* describes how many observations have been used to calculate the mean temperature that day. While the temperature itself is of obvious interest, this number is unlikely to bear any relevant information. This is true for any such variable (all contain 'sample' in their name) and thus we can remove them from the data.\n",
    "\n",
    "Additionally, we remove the columns related to max_temperature as the documentation states that \"*the time that this value is reported differs by country and region, so this value will sometimes not be the maximum for the calendar day.\"* This measurement error can bias our results severly as we can expect temperature to be a strong predictor of the snowing conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all columns containing the words 'sample' or 'max_temperature' can be removed \n",
    "sample_cols = [col for col in low_missings.columns if 'sample' in col]\n",
    "max_temp_cols = [col for col in low_missings.columns if 'max_temp' in col]\n",
    "final = low_missings.drop(sample_cols+max_temp_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have removed features with a high percentage of missings, we still have observations in our data that contain missing values for specific variables. In this case, we simply remove all rows with missing values in any feature as we end up with a reasonable amount of data. For notes on a potentially better approach (imputation) see the *Notes on Improvement* below.\n",
    "\n",
    "To make sure that we are not deleting observations in which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'station_number': (0, 2142), 'wban_number': (0, 2142), 'year': (0, 2142), 'month': (0, 2142), 'day': (0, 2142), 'mean_temp': (0, 2142), 'mean_dew_point': (0, 2142), 'mean_visibility': (0, 2142), 'mean_wind_speed': (2, 2140), 'max_sustained_wind_speed': (3, 2139), 'total_precipitation': (55, 2087), 'fog': (0, 2142), 'rain': (0, 2142), 'snow': (0, 2142), 'hail': (0, 2142), 'thunder': (0, 2142), 'tornado': (0, 2142), 'date': (0, 2142)}\n"
     ]
    }
   ],
   "source": [
    "snow_balance = {}\n",
    "for feat in final.columns:\n",
    "    share_missing_feat = final[(final[feat].isnull())&(final['snow']==1)].shape[0]\n",
    "    share_existing_feat = final[(final[feat].isnull()==False)&(final['snow']==1)].shape[0]\n",
    "    snow_balance[feat]=(share_missing_feat, share_existing_feat)\n",
    "print(snow_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on improvement:** Here are some thoughts on how preprocessing and the feature selection could be improved:\n",
    "\n",
    "- impute missing values\n",
    "    - right now we are losing observations because they do not contain information on a small subset of features; we could improve this by imputing these missing values \n",
    "    - I have no experience in imputation yet and cannot state anything reliably on the effects this might have on predictions but e.g. using mean imputation for weather data could bias our results in seasonal regions; looking at the variance of these features first could help assess how well imputation approaches might work or bias the results (?)\n",
    "- inspect correlation of features with outcome to assess relevance of feature\n",
    "- run a simple model (e.g. Lasso Regression, (pruned) Decision Tree/Random Forest) that allows for feature selection \n",
    "    - e.g. we could run a Lasso Regression on a random (bootstrapped) subsample of the data to retrieve information on which variables are relevant to include\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Task\n",
    "**Now it is time to split the data, into a training, evaluation and test set. As a reminder, the date we are trying to predict snow fall for is the following, and hence should constitute your test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-06-24\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "#save test date to create test set in next cell\n",
    "test_date = str(dt.datetime.today()- dt.timedelta(days=13*365)).split(' ')[0]\n",
    "#turn test_date into datetime object\n",
    "test_date = dt.datetime.strptime(test_date, '%Y-%m-%d').date()\n",
    "print(test_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define our outcome and some lists containing column names that we need for further data manipulation but should not be included as features in our prediction. Namely, these are meta information such as the date column, year, day and the two columns containing station numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'snow'\n",
    "meta_cols = ['year', 'day', 'month', 'date', 'station_number', 'wban_number']\n",
    "#transform the outcome variable from bool to integer \n",
    "final[outcome]=final[outcome].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I split up the data into training, evaluation and test set. We start with the test set by only keeping observations on *test_date* (as of writing: 2009-06-23).\n",
    "\n",
    "Next, we split up the pandas.DataFrame object into X and y, where y is the outcome variable to be predicted. X should not contain variables such as the station number and features related to the date (except month). We exclude these variables as they are unlikely to contain any relevant information that helps predict the likelihood of snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final[final['date']==test_date]\n",
    "X_test = test.drop([outcome]+meta_cols, axis = 1)\n",
    "y_test = test[outcome]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we turn to splitting up the remaining observations into a training and an evaluation set. The latter will be used to fine-tune models and decide which model to use for final predictions. Similarly to the test set, columns without relevant information are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore test date when creating training and evaluation set and remove irrelevant features\n",
    "X = final[final['date']!=test_date].drop([outcome]+meta_cols, axis = 1)\n",
    "y = final[final['date']!=test_date][outcome]\n",
    "#split up into train and evaluation set \n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "**If you made it up to here all by yourself, you can use your prepared dataset to train an Algorithm of your choice to forecast whether it will snow on the following date for each station in this dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009-06-24'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "str(dt.datetime.today()- dt.timedelta(days=13*365)).split(' ')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are allowed to use any library you are comfortable with such as sklearn, tensorflow keras etc. \n",
    "If you did not manage to finish part one feel free to use the data provided in 'coding_challenge.csv' Note that this data does not represent a solution to Part 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training and evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#plotting\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines functions that are used to train models and evaluate their performance. For more details on what each function does, what inputs they use and what they return, see respective docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for training models\n",
    "def CV_fit(est, X, y, params):\n",
    "    '''\n",
    "    Use cross-validation grid search to find \"best\" estimator given a set of parameters.\n",
    "    \n",
    "    *est: sklearn estimator \n",
    "    *X: array-like\n",
    "        Contains features used for prediction.\n",
    "    *y: 1D array-like+\n",
    "        Contains outcome to be predicted, must be binary (!)\n",
    "\n",
    "    Returns: GridSearchCV.best_estimator_\n",
    "    '''\n",
    "    #first check if outcome supplied is binary \n",
    "    #! not yet working correctly\n",
    "    # if ((y==0) | (y==1)).all()==False:\n",
    "    #     raise ValueError(f'y must be binary.')\n",
    "    gs_cv = GridSearchCV(est, param_grid = params)\n",
    "    gs_cv.fit(X, y)\n",
    "    \n",
    "    return gs_cv.best_estimator_\n",
    "\n",
    "def eval_predict(true, pred):\n",
    "    '''\n",
    "    Calculate MSE and F1-Score. Print and return results.\n",
    "    '''\n",
    "    mse = np.mean((pred-true)**2)\n",
    "    #need to set zero_division to 1 in our case as test set only contains 0s\n",
    "    f1 = f1_score(true, pred, zero_division = 1)\n",
    "    \n",
    "    res_str = f'MSE: {mse} \\n F1-Score: {f1}'\n",
    "    print(res_str)\n",
    "    \n",
    "    return mse, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the snowing conditions on the target date we train a random forest classifier. Its advantage is its ease of implementation (time-efficient) and the fact that it can handle unscaled data well. Moreover, random forests implement non-linear estimation without pre-defining the degrees of interaction or polynomials (as would be necessary for linear classifiers for example) and it has a proven track-record of performing quite well as an out-of-the-box algorithm. Implementation of more sophisticated techniques would be subject to a more time-demanding procedure.\n",
    "\n",
    "Each model is then evaluated based on the evaluation set and the final prediction of the test data is made based upon the best model. Although it might seem better for this specific prediction to use the test set to choose the best model, the fact that the test set only contains one observation makes the evaluation prone to result in a model too specifically focused on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with a simple grid search random forest classifier and use all available features \n",
    "#set up parameter dictionary \n",
    "params = {'max_depth': range(2, 10, 5), 'min_samples_split': range(10, 40, 5), 'min_samples_leaf': range(25, 50, 5)}\n",
    "#get best RFC\n",
    "RFC = RandomForestClassifier()\n",
    "best_RFC = CV_fit(est = RFC, X = X_train, y = y_train, params = params)\n",
    "#predict y_eval using fitted model\n",
    "y_eval_RFC = best_RFC.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0 \n",
      " F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "#calculate evaluation metrics\n",
    "mse_RFC, f1_RFC = eval_predict(y_eval, y_eval_RFC)\n",
    "#bind metrics into a dataframe\n",
    "metrics_dict = {'model': 'RFC', \n",
    "                'MSE': mse_RFC,\n",
    "                'F1': f1_RFC}\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict, orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are quite odd as the model performs perfectly on the evaluation set. Usually, this might suggest problems of overfitting but the optimal *max_depth* parameter oafter cross-validation is 2, i.e. trees are extremely shallow. Taking a look at the feature importace (mean decrease in impurity by splitting on this feature across all trees) we see that features such as temperature and wind speed do not play any role in predicting the weather outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mean_temp', 0.0), ('mean_dew_point', 0.0), ('mean_visibility', 0.010332645634549532), ('mean_wind_speed', 0.0), ('max_sustained_wind_speed', 0.0), ('total_precipitation', 0.0029535120466042206), ('fog', 0.27896161438030476), ('rain', 0.18194958970608957), ('hail', 0.14973876242463618), ('thunder', 0.16606387580781573), ('tornado', 0.21)]\n"
     ]
    }
   ],
   "source": [
    "print([(feat, imp) for feat, imp in zip(X_train.columns,best_RFC.feature_importances_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reason might be the extremely selective subsample we are using. In case weather stations with similar numbers are close to each other, our model is likely to not be very generalizable to predicting snowing conditions around the globe but rather only in one specific area. \n",
    "\n",
    "To test this hypothesis, we create a new evaluation set which contains stations with numbers between 726000 and 726284 and evaluate the models performance for these stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0 \n",
      " F1-Score: 1.0\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "#create a new eval set\n",
    "new_eval = df[(df['station_number']>726000)&(df['station_number']<726284)]\n",
    "#only keep stations that appear in 2005 to 2009\n",
    "years = 5\n",
    "sn_years_new_eval = new_eval[['station_number', 'year']].groupby(['station_number']).nunique()\n",
    "#only keep the stations where count of years == years\n",
    "stations_all_years_new_eval = list(sn_years_new_eval[sn_years_new_eval['year']==years].index)\n",
    "new_eval = new_eval[new_eval['station_number'].isin(stations_all_years_new_eval)]\n",
    "new_eval = new_eval[keep_cols]\n",
    "new_eval = new_eval.drop(max_temp_cols+sample_cols, axis = 1)\n",
    "new_eval = new_eval.dropna()\n",
    "#then predict new evaluation set outcomes and measure performance\n",
    "y_new_eval = new_eval[outcome].astype(int)\n",
    "X_new_eval = new_eval.drop([outcome]+meta_cols, axis = 1)\n",
    "y_new_eval_RFC = best_RFC.predict(X_new_eval)\n",
    "mse_new, f1_new = eval_predict(y_new_eval, y_new_eval_RFC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of the predictor has shown it to be performing outstandingly well and we now predict the snowing conditions in the test set, i.e. on the target date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0 \n",
      " F1-Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/llccf/OneDrive/Dokumente/Bewerbungen/CodingChallenges/7Learnings/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_RFC = best_RFC.predict(X_test)\n",
    "eval_predict(y_test, y_test_RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "619b8c47a6a05fcdc40a2b23d4125b9beb65736f2d83c7c0dca94c5b37e5a4b3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
